TODO
====
* Finished for the first update:
  * Multiple hare objects in visualization functions
    * Precision/recall add: everything yes no
  * A really working classifier
    * Implement: downloading pretrained models
      * Extend config: title, mirrors, hash
    * Retrain the simple MOBA classifier
    * Find some working examples
  * Investigate: what would be a good publication subject

* Better errors when dependency importing

* Experiment with neural nets:
  * Study best model in Kaggle... what is different from this experiment? https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52557
    * Check: how is it possible that with less data, scores are higher?
    * Combine embeddings

  * See if this best result can be improved by using Ulmfit http://nlp.fast.ai/ or Bert https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
    * https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/

  * Look for others studies, like from the workshop Iris mentioned... do they have interesting architectures? https://sites.google.com/view/alw3/past-workshops?authuser=0

* Look up: what exactly is ConvoKit?