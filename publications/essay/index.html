<html>
<head>
<title>Catching cyberbullies in the act with neural networks</title>
<script src="js/detector_visualization.js"></script>
<script src="js/activations_visualization.js"></script>
<script src="precalculated_data/activations.js"></script>
<link rel="stylesheet" href="style.css"></head>
<link href="https://fonts.googleapis.com/css?family=Montserrat:400,600,700&amp;display=swap" rel="stylesheet">
<body>

<h1>Catching cyberbullies in the act with neural networks</h1>

<div class="main">
	<p class="lead">Digital harassment is a problem in many corners of the internet. In this article you can play with techniques to automatically detect bad actors, preferably as early in the conversation as possible. We will see that neural networks work better than simple words lists, but also that they are black boxes; we attempt to give you some insights into what the various parts of the network use to come to their decisions.</p>

	<p><span id="first_character">A</span>ccording to a 2016 report '47% of internet users have experienced online harassment or abuse [1], and 27% of all American internet users self-censor what they say online because they are afraid of being harassed. On a similar note, a survey by The Wikimedia Foundation (behind Wikipedia and other things) showed that 38% of the editors had encountered harassment, and over half them said this lowered their motivation to contribute in the future [2]. If we want safe and productive online platforms where users do not chase each other away, something needs to be done.</p>

	<p>The best solution to this problem might be to use human moderators that read everything take action when somebody crosses a boundary, but this is not always feasable; for example, in popular online games hundreds of conversations are simultaneously happening all the time. At the same time, some online games are notorious for their toxic community. For example, the game League of Legends has been called "[x]". This is a typical conversation from the game:</p>

	<p></p>

	<p>For this article, we will therefore use a dataset of conversations from this game and try to see if we can separate 'toxic' players from 'normal' players automatically. Let's take 10 conversations where 1 person misbehaves as an example, and see if we can pinpoint this 1 player, preferably early in the conversation. A first approach for an automated detector might be to use a simple list of swear words and insults like 'fuck','suck', 'noob' and 'fag', and label a player as toxic if s/he uses it more often than a particular threshold. Below, you can slide through a conversation and see how detectors with thresholds of 1, 2, 3 and 5 bad words perform.</p>

	<div id="dictionary_detectors"></div>

	<p>As you can see, the detector with the low threshold detects all toxic players early in the game, but has lots of false positives. On the other hand, the detector with the high threshold does not have this problem, but misses a lot of toxic players (false negatives). This tension between false positive and false negatives is a problem any approach will have; our goal is to find approach where it is as small as possible.</p>

	<p>A better solution might be to use machine learning: we give thousands of examples of conversations with toxic players to a training algorithm and ask it to figure out how to recognize harassment by itself. Of course, such an algorithm will learn that swear words and insults are good predictors for toxicity, but it can also pick up more subtle word combinations and other phenomena. For example, you might have noticed that the average toxic player is speaking a lot more than the others.</p>

	<p>The most successful algorithms in tasks like this these days are so-called neural networks. While even experts have trouble fully understanding why exactly they are so successful, there are some techniques to look under the hood and see what the network has learned exactly. For example, you can put all the words a network has learned something about in a 3D space in such a way that words that are (according to the network) closer together are also closer in meaning.</p>

	<div>3D</div>

	<p>We see that...</p>

	<p>Another thing we can look at is what the individual <em>neurons</em> do; you can think of a neuron as an individual worker that during the training phase tries to find a simple meaningful task for itself. A neural network is basically a collection of these workers, all doing simple but different tasks. What really sets a neural network apart from a simple word list based detector is that these workers are organized in <em>layers</em>. Neurons in the lower layers typically pick up a small concrete task, like recognizing particular words, while the higher layers do something increasingly more abstract, like monitoring the temperature of the conversation as a whole. In the interactive visualization below, you can see where in the conversation which neurons activate.</p>

	<div class="interactive_panel" id="neuron_activations"></div>

	<p>We see that the neurons at the lower level have learned to activate on 1 or more words. [example] . The neurons at the higher levels, on the other hand, activate on such words but then slowly fade out, as if encountering such a word makes them more suspicious for some time.</p>

	<p>The big question of course is whether a harassment detector using a neural network instead of word list performs better. Below you can compare the previous word list based approach against three neural networks with different thresholds. The threshold now is the network's <em>confidence</em>: a number between 0 and 1 indicating how sure the network is a particular player is toxic.</p>

	<div id="neural_detectors"></div>

	<p>We see that.</p>

	<p>Besides these technical challenges, there are a number of ethical questions too... do we only want to use this technique to study the toxicity within a community, or do we really want to put it in production and monitor conversations in real time? And if so, what does it mean for a person or a whole community have a real time watch dog robot, always looking over your shoulder? What should be done when it detects somebody? Should s/he be notified, warned, muted, banned? Related to that, the former director of Riot Games' Player Behavior Unit attributes most toxicity to 'the average person just having a bad day'... Is labeling a whole person as toxix or non-toxic not too simplistic?</p>

	<p>Ending</p>

	<div id="playground"></div>
</div>

<script>
loadPrecalculatedData(function()
{
	var element = document.getElementById('dictionary_detectors');
	var detectors = [['dic',0],['dic',1],['dic',2]];
	initializeDetectorVisualization(element,'dictionary_detectors',detectors,false);

	element = document.getElementById('neural_detectors');
	detectors = [['dic',1],['bigru_embeddings',4],['bigru_embeddings',8],['bigru_embeddings',10]];
	initializeDetectorVisualization(element,'neural_detectors',detectors,false);

	element = document.getElementById('playground');
	detectors = [];
	initializeDetectorVisualization(element,'playground',detectors,true);
});

var neuronActivationsVisualization = new NeuronActivationsVisualization(document.getElementById('neuron_activations'),
															activations,[0,1,2],[3,4,5]);

</script>

</body>
</html>